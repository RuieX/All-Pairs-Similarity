{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# All Pairs Similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO:\n",
    "find all docs that are at least 75% (varying threshold) similar\n",
    "measure similarity with cosine similarity (dot product $d_i$, $d_j$)\n",
    "(we want the exact similarity, but can also use SimHash for approximation as extra)\n",
    "(also PCA?, Clustering, Sketching, for efficiency or whatever)\n",
    "(compare the tradeoff, quality against time)\n",
    "\n",
    "2 python array for dot product, not lists (how come)\n",
    "python-numpy: sequential implementation, parallel implementation (MapReduce, Apache Spark)\n",
    "\n",
    "Report 2 pp: discuss the performance figures of the parallel implementation\n",
    "varying datasets, similarity threshold,\n",
    "varying number of workers\n",
    "varying thresholds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Docs all pairs similarity:\n",
    "\n",
    "A document is a vector $d$ of $N$ elements:\n",
    "❏ $N$ is the number of distinct words in the corpus (the lexicon)\n",
    "❏ $d[i]$: stores the frequency of the term $i$ in document $d(tf(i))$. Then $d$ is normalized (divided by its $L_2$ norm)\n",
    "❏ additionally you can use $\\text{tf-idf}$:\n",
    "$$\n",
    "\\text{tf-idf}(i) = tf(i) * ln(\\frac{N_{docs}}{df(i)})\n",
    "$$\n",
    "\n",
    "There are many ways to measure similarity\n",
    "❏ Cosine:\n",
    "$$\n",
    "s(a,b) = \\sum_{i=1...N} a[i] * b[i]\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sequential algorithm\n",
    "\n",
    "very expensive -> $n^2$\n",
    "\n",
    "Given minimum similarity threshold $\\tau$:\n",
    "\n",
    "SIM_DOCS = 0\n",
    "For-each document $d_1$ in the corpus D :\n",
    "___For-each document $d_2$ in the corpus D :\n",
    "______if $d_1$!=$d_2$ and s($d_1$, $d_2$) >= $\\tau$ : //compute $similarity(d_i,d_j)$\n",
    "_________SIM_DOCS += 1\n",
    "\n",
    "\n",
    "Datasets\n",
    "[beir](https://github.com/beir-cellar/beir)\n",
    "[movielens](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "tip:\n",
    "- we are interested to the similar document pairs, not the number of similar document\n",
    "- we can skip short docs because short vs long cannot five high similarity\n",
    "- sort by length (exploit the length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}