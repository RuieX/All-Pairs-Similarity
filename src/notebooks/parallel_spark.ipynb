{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "from src.utilities.similarity_parallel_spark import pyspark_APDS\n",
    "import pandas as pd\n",
    "from typing import Tuple, Type, List, Dict, Any"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets = ['nfcorpus'] # Choosen datasets\n",
    "thresholds: List[float] = [0.5, 0.6, 0.7, 0.8, 0.9] # Choosen thresholds\n",
    "numslices_factor = [1, 2, 5, 10, 15] # Choosen numslices factors N_PARTITIONS\n",
    "max_workers = [1, 2, 5, 10, 15] # n_executors\n",
    "considered_docs = 750\n",
    "\n",
    "# # Download datasets\n",
    "# datasets_data = {dataset: download_dataset(dataset) for dataset in datasets}\n",
    "#\n",
    "# # Pre-process and sample with the original datasets\n",
    "# pre_processed_data = {dataset: sample_dict(documents_preprocessing(dataset, docs_dict), considered_docs)\n",
    "#                       for dataset, docs_dict in datasets_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "pyspark_results = []\n",
    "\n",
    "for ds_name, sampled_dict in pre_processed_data.items():\n",
    "\n",
    "    print(f'\\n------------ALL PAIRS DOCUMENTS SIMILARITY - {ds_name}------------')\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        print(f'\\n--------Running with threshold: {threshold}--------')\n",
    "\n",
    "        for s_factor, workers in itertools.product(numslices_factor, range(1, max_workers + 1)):\n",
    "            # PySpark Execution\n",
    "\n",
    "            print(f'\\nPySpark Parallel Execution with {workers} workers and slice factor of {s_factor}')\n",
    "            sim_doc_ps, ps_res = pyspark_APDS(ds_name=ds_name, sampled_dict=sampled_dict, threshold=threshold, workers=workers, s_factor=s_factor)\n",
    "            pyspark_results.append(ps_res)\n",
    "            create_doc_sim_csv(sim_doc_ps, ds_name, threshold, None, workers)\n",
    "            print(' Done')\n",
    "\n",
    "        print('\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print('\\nSaving pyspark_results')\n",
    "pd.DataFrame.from_dict(\n",
    "    dict(zip(range(len(pyspark_results)), pyspark_results)),\n",
    "    orient='index',\n",
    "    columns=[\n",
    "        'ds_name',\n",
    "        'elapsed',\n",
    "        'threshold',\n",
    "        'uniqie_pairs_sim_docs',\n",
    "        'workers',\n",
    "        'slice_factor'\n",
    "    ],\n",
    ").to_csv('./results/pyspark_results.csv', index=False)\n",
    "print( 'Done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}