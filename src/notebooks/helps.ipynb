{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 3 - All Pair Documents Similarity Search"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Follow instructions in README.md to setup local standalone cluster, which is required to run this notebook."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a/miniforge3/envs/sbruch-assignment-Lwmd/lib/python3.10/site-packages/beir/util.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/a/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/a/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/a/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "from typing import Tuple, Type, List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Needed to correctly set the python executable of the current conda env\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['SPARK_LOCAL_IP'] = \"127.0.0.1\"\n",
    "\n",
    "# UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set.\n",
    "# It is required to set this environment variable to '1' in both driver and executor\n",
    "#   sides if you use pyarrow>=2.0.0.\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "MASTER_HOST = \"localhost\"  # master host in local standalone cluster\n",
    "\n",
    "import pyspark.sql as psql\n",
    "\n",
    "# IMPORTANT: create session prior to importing pyspark.pandas, else\n",
    "#   spark won't use all specified cores\n",
    "from src.utilities.utils import AVAILABLE_CORES, AVAILABLE_RAM_GB\n",
    "\n",
    "import src.tokenization as tok\n",
    "import src.apdss.map_reduce as mr\n",
    "import src.apdss.sequential as seq\n",
    "import src.utilities.io as io\n",
    "from src.apdss.core import Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## APDSS - Computing Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notes:\n",
    "- Driver is allocated 2 cores and 2 gb of RAM\n",
    "- Executors are left with 64gb of RAM and 30 cores to share\n",
    "- Sim Threshold is fixed at 0.95 because it didn't seem to impact computation times\n",
    "- trec-covid-small corpus is a custom-sample of the first 13.5k docs from the trec-covid corpus (170k docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SeqSetting: Type = Tuple[str, float]\n",
    "\"\"\"Dataset name\"\"\"\n",
    "\n",
    "MRSetting: Type = Tuple[str, int, float]\n",
    "\"\"\"(dataset_name, n_executors, threshold)\"\"\"\n",
    "\n",
    "# Full settings\n",
    "datasets: List[str] = [\"SciFact\"]\n",
    "n_executors: List[int] = [1, 2, 5, 10, 20]\n",
    "thresholds: List[float] = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Since the datasets used are relatively small, one partition per core is sufficient\n",
    "# The rule of thumb would be \"numPartitions = numWorkers * cpuCoresPerWorker\"\n",
    "# In my case, local standalone cluster, there is just 1 worker with AVAILABLE_CORES cores\n",
    "# See this answer for a useful discussion about how to determine numPartitions\n",
    "#   https://stackoverflow.com/a/39398750/19582401\n",
    "N_PARTITIONS = AVAILABLE_CORES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus_paths: Dict[str, str] = {\n",
    "    d: io.get_corpus_path(d)\n",
    "    for d in datasets\n",
    "}\n",
    "\n",
    "# , \"\", \"\", \"\"\n",
    "j\n",
    "# Custom samples from trec-covid dataset\n",
    "datasets.append(\"trec-covid-long\")\n",
    "datasets.append(\"trec-covid-short\")\n",
    "datasets.append(\"trec-covid-random\")\n",
    "corpus_paths[\"trec-covid-long\"] = os.path.join(io.DATA_DIR, \"trec-covid\", \"samples\", \"longest_docs.pkl\")\n",
    "corpus_paths[\"trec-covid-long\"] = os.path.join(io.DATA_DIR, \"trec-covid\", \"samples\", \"shortest_docs.pkl\")\n",
    "corpus_paths[\"trec-covid-long\"] = os.path.join(io.DATA_DIR, \"trec-covid\", \"samples\", \"random_docs.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_spark_session(n_executors: int, app_name: str) -> psql.SparkSession:\n",
    "    driver_ram_gb = 2\n",
    "    driver_cores = 2\n",
    "    mem_per_executor = (AVAILABLE_RAM_GB - driver_ram_gb) // n_executors\n",
    "    cores_per_executor = (AVAILABLE_CORES - driver_cores) // n_executors\n",
    "\n",
    "    logger.debug(f\"Executor memory: {mem_per_executor}\")\n",
    "    logger.debug(f\"AVAILABLE_RAM_GB: {AVAILABLE_RAM_GB}\")\n",
    "    logger.debug(f\"Total executor memory: {(AVAILABLE_RAM_GB - driver_ram_gb)}\")\n",
    "    logger.debug(f\"Executor cores: {cores_per_executor}\")\n",
    "\n",
    "\n",
    "    spark: psql.SparkSession = (\n",
    "        psql.SparkSession.builder\n",
    "        .master(f\"spark://{MASTER_HOST}:7077\")  # connect to previously started master host\n",
    "\n",
    "        .appName(f\"{app_name}\")\n",
    "        #.config(\"spark.driver.host\", f\"{MASTER_HOST}:7077\")\n",
    "        .config(\"spark.driver.cores\", driver_cores)\n",
    "        .config(\"spark.driver.memory\", f\"{driver_ram_gb}g\")\n",
    "        .config(\"spark.executor.instances\", n_executors)\n",
    "        .config(\"spark.executor.cores\", cores_per_executor)\n",
    "        .config(\"spark.executor.memory\", f\"{mem_per_executor}g\")\n",
    "        .config(\"spark.default.parallelism\", AVAILABLE_CORES)\n",
    "        .config(\"spark.cores.max\", AVAILABLE_CORES - driver_cores)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    # Add local dependencies (local python source files) to SparkContext and sys.path\n",
    "    src_zip_path = os.path.abspath(\"../../src.zip\")\n",
    "    logger.debug(f\"Adding {src_zip_path} to SparkContext\")\n",
    "\n",
    "    spark.sparkContext.addPyFile(src_zip_path)\n",
    "    sys.path.insert(0, SparkFiles.getRootDirectory())\n",
    "\n",
    "    return spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mr_results: Dict[MRSetting, Results] = {}\n",
    "seq_results: Dict[SeqSetting, Results] = {}\n",
    "\n",
    "OUT_DIR = \"../../out\"\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.mkdir(OUT_DIR)\n",
    "\n",
    "DATASET_NAME_KEY = \"d_name\"\n",
    "N_EXECS_KEY = \"n_execs\"\n",
    "THRESHOLD_KEY = \"threshold\"\n",
    "TIME_KEY = \"time\"\n",
    "SIM_DOCS_KEY = \"time\"\n",
    "\n",
    "def save_mr_results():\n",
    "    with open(f\"{OUT_DIR}/mr_results.json\", \"w\") as f:\n",
    "        data = {}\n",
    "        for i, settings in enumerate(mr_results.keys()):\n",
    "            d_name, n_execs, t = settings\n",
    "            res = mr_results[settings]\n",
    "            data[i] = {\n",
    "                DATASET_NAME_KEY: d_name,\n",
    "                N_EXECS_KEY: n_execs,\n",
    "                THRESHOLD_KEY: t,\n",
    "                SIM_DOCS_KEY: len(res.similar_docs),\n",
    "                TIME_KEY: res.time\n",
    "            }\n",
    "\n",
    "        json.dump(data, f)\n",
    "\n",
    "def save_seq_results():\n",
    "    with open(f\"{OUT_DIR}/seq_results.json\", \"w\") as f:\n",
    "        data = {}\n",
    "        for i, settings in enumerate(seq_results.keys()):\n",
    "            d_name, t = settings\n",
    "            res = seq_results[settings]\n",
    "            data[i] = {\n",
    "                DATASET_NAME_KEY: d_name,\n",
    "                THRESHOLD_KEY: t,\n",
    "                SIM_DOCS_KEY: len(res.similar_docs),\n",
    "                TIME_KEY: res.time\n",
    "            }\n",
    "\n",
    "        json.dump(data, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for d_name in datasets:\n",
    "    # Create \"single-use\" spark session to parse the text\n",
    "    spark = create_spark_session(n_executors=4, app_name=\"Doc Features\")\n",
    "    docs_scores_df = tok.get_document_features(\n",
    "        spark=spark,\n",
    "        corpus_json_path=corpus_paths[d_name],\n",
    "        n_partitions=N_PARTITIONS\n",
    "    )\n",
    "    docs_scores_pandas: pd.DataFrame = docs_scores_df.toPandas()\n",
    "    spark.stop()\n",
    "\n",
    "    for t in thresholds:\n",
    "        for n_execs in n_executors:\n",
    "            # ----- MR -----------------------------------\n",
    "            spark = create_spark_session(n_executors=n_execs, app_name=\"MR-APDSS\")\n",
    "\n",
    "            # Create docs df from pandas instance with new context\n",
    "            docs_scores_df = spark.createDataFrame(docs_scores_pandas)\n",
    "            docs_scores_df.cache()\n",
    "\n",
    "            mr_res = mr.MapReduceAPDSS().apdss(\n",
    "                spark=spark,\n",
    "                docs_scores_df=docs_scores_df,\n",
    "                threshold=t,\n",
    "                num_partitions=N_PARTITIONS\n",
    "            )\n",
    "\n",
    "            mr_setting = (d_name, n_execs, t)\n",
    "            logger.info(f\"MR::({mr_setting}):time -> {mr_res.time}\")\n",
    "            mr_results[mr_setting] = mr_res\n",
    "            save_mr_results()  # Overwrite each time to make sure data isn't lost\n",
    "\n",
    "            # Stop context so that next iteration creates a new one with\n",
    "            #   a different number of executors\n",
    "            spark.stop()\n",
    "\n",
    "        # ----- SEQ -----------------------------------\n",
    "        # Create docs df from pandas instance with new context\n",
    "        spark = create_spark_session(n_executors=4, app_name=\"SEQ-APDSS\")\n",
    "        docs_scores_df = spark.createDataFrame(docs_scores_pandas)\n",
    "\n",
    "        seq_res = seq.SequentialAPDSS().apdss(\n",
    "            spark=spark,\n",
    "            docs_scores_df=docs_scores_df,\n",
    "            threshold=t\n",
    "        )\n",
    "\n",
    "        seq_setting = (d_name, t)\n",
    "        logger.info(f\"SEQ::({seq_setting}):time -> {seq_res.time}\")\n",
    "        seq_results[seq_setting] = seq_res\n",
    "        save_seq_results()  # Overwrite each time to make sure data isn't lost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mr_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_mr_results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seq_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_seq_results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "sbruch-assignment-lwmd",
   "language": "python",
   "display_name": "lwmd_asgmt_sbruch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}