{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 3 - All Pair Documents Similarity Search"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Follow instructions in README.md to setup local standalone cluster, which is required to run this notebook."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a/miniforge3/envs/sbruch-assignment-Lwmd/lib/python3.10/site-packages/beir/util.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/a/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/a/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/a/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "from typing import Tuple, Type, List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Needed to correctly set the python executable of the current conda env\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['SPARK_LOCAL_IP'] = \"127.0.0.1\"\n",
    "\n",
    "# UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set.\n",
    "# It is required to set this environment variable to '1' in both driver and executor\n",
    "#   sides if you use pyarrow>=2.0.0.\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "\n",
    "MASTER_HOST = \"localhost\"  # master host in local standalone cluster\n",
    "\n",
    "import pyspark.sql as psql\n",
    "\n",
    "# IMPORTANT: create session prior to importing pyspark.pandas, else\n",
    "#   spark won't use all specified cores\n",
    "from src.utilities.utils import AVAILABLE_CORES, AVAILABLE_RAM_GB\n",
    "\n",
    "import src.tokenization as tok\n",
    "import src.apdss.map_reduce as mr\n",
    "import src.apdss.sequential as seq\n",
    "import src.utilities.io as io\n",
    "from src.apdss.core import Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## APDSS - Computing Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notes:\n",
    "- Driver is allocated 2 cores and 2 gb of RAM\n",
    "- Executors are left with 64gb of RAM and 30 cores to share\n",
    "- Sim Threshold is fixed at 0.95 because it didn't seem to impact computation times\n",
    "- trec-covid-small corpus is a custom-sample of the first 13.5k docs from the trec-covid corpus (170k docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "MRSetting: Type = Tuple[str, int, float]\n",
    "\"\"\"(dataset_name, n_executors, threshold)\"\"\"\n",
    "\n",
    "# Full settings\n",
    "datasets: List[str] = []\n",
    "n_executors: List[int] = [1, 2, 5, 10, 15]\n",
    "thresholds: List[float] = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Since the datasets used are relatively small, one partition per core is sufficient\n",
    "# The rule of thumb would be \"numPartitions = numWorkers * cpuCoresPerWorker\"\n",
    "# In my case, local standalone cluster, there is just 1 worker with AVAILABLE_CORES cores\n",
    "# See this answer for a useful discussion about how to determine numPartitions\n",
    "#   https://stackoverflow.com/a/39398750/19582401\n",
    "N_PARTITIONS = AVAILABLE_CORES"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "corpus_paths: Dict[str, str] = {\n",
    "    d: io.get_corpus_path(d)\n",
    "    for d in datasets\n",
    "}\n",
    "\n",
    "# Custom samples from trec-covid dataset\n",
    "datasets.append(\"trec-covid-long\")\n",
    "datasets.append(\"trec-covid-short\")\n",
    "datasets.append(\"trec-covid-random\")\n",
    "corpus_paths[\"trec-covid-long\"] = os.path.join(io.DATA_DIR, \"trec-covid\", \"samples\", \"longest_docs.jsonl\")\n",
    "corpus_paths[\"trec-covid-long\"] = os.path.join(io.DATA_DIR, \"trec-covid\", \"samples\", \"shortest_docs.jsonl\")\n",
    "corpus_paths[\"trec-covid-long\"] = os.path.join(io.DATA_DIR, \"trec-covid\", \"samples\", \"random_docs.jsonl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def create_spark_session(n_executors: int, app_name: str) -> psql.SparkSession:\n",
    "    driver_ram_gb = 2\n",
    "    driver_cores = 2\n",
    "    mem_per_executor = (AVAILABLE_RAM_GB - driver_ram_gb) // n_executors\n",
    "    cores_per_executor = (AVAILABLE_CORES - driver_cores) // n_executors\n",
    "\n",
    "    logger.debug(f\"Executor memory: {mem_per_executor}\")\n",
    "    logger.debug(f\"AVAILABLE_RAM_GB: {AVAILABLE_RAM_GB}\")\n",
    "    logger.debug(f\"Total executor memory: {(AVAILABLE_RAM_GB - driver_ram_gb)}\")\n",
    "    logger.debug(f\"Executor cores: {cores_per_executor}\")\n",
    "\n",
    "\n",
    "    spark: psql.SparkSession = (\n",
    "        psql.SparkSession.builder\n",
    "        .master(f\"spark://{MASTER_HOST}:7077\")  # connect to previously started master host\n",
    "\n",
    "        .appName(f\"{app_name}\")\n",
    "        #.config(\"spark.driver.host\", f\"{MASTER_HOST}:7077\")\n",
    "        .config(\"spark.driver.cores\", driver_cores)\n",
    "        .config(\"spark.driver.memory\", f\"{driver_ram_gb}g\")\n",
    "        .config(\"spark.executor.instances\", n_executors)\n",
    "        .config(\"spark.executor.cores\", cores_per_executor)\n",
    "        .config(\"spark.executor.memory\", f\"{mem_per_executor}g\")\n",
    "        .config(\"spark.default.parallelism\", AVAILABLE_CORES)\n",
    "        .config(\"spark.cores.max\", AVAILABLE_CORES - driver_cores)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    # Add local dependencies (local python source files) to SparkContext and sys.path\n",
    "    src_zip_path = os.path.abspath(\"../../src.zip\")\n",
    "    logger.debug(f\"Adding {src_zip_path} to SparkContext\")\n",
    "\n",
    "    spark.sparkContext.addPyFile(src_zip_path)\n",
    "    sys.path.insert(0, SparkFiles.getRootDirectory())\n",
    "\n",
    "    return spark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "mr_results: Dict[MRSetting, Results] = {}\n",
    "\n",
    "OUT_DIR = \"../../out\"\n",
    "if not os.path.exists(OUT_DIR):\n",
    "    os.mkdir(OUT_DIR)\n",
    "\n",
    "DATASET_NAME_KEY = \"d_name\"\n",
    "N_EXECS_KEY = \"n_execs\"\n",
    "THRESHOLD_KEY = \"threshold\"\n",
    "TIME_KEY = \"time\"\n",
    "SIM_DOCS_KEY = \"time\"\n",
    "\n",
    "def save_mr_results():\n",
    "    with open(f\"{OUT_DIR}/mr_results.json\", \"w\") as f:\n",
    "        data = {}\n",
    "        for i, settings in enumerate(mr_results.keys()):\n",
    "            d_name, n_execs, t = settings\n",
    "            res = mr_results[settings]\n",
    "            data[i] = {\n",
    "                DATASET_NAME_KEY: d_name,\n",
    "                N_EXECS_KEY: n_execs,\n",
    "                THRESHOLD_KEY: t,\n",
    "                SIM_DOCS_KEY: len(res.similar_docs),\n",
    "                TIME_KEY: res.time\n",
    "            }\n",
    "\n",
    "        json.dump(data, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# spark.stop()\n",
    "import os.path\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import src.utilities.io as io\n",
    "import src.utilities.preprocess as pp\n",
    "import src.utilities.similarity_sequential as sim_seq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-26 21:28:42 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/171332 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43c0544c05ae4f9a94e8e8a6276cff32"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-26 21:28:48 - Loaded 171332 TEST Documents.\n",
      "2023-05-26 21:28:48 - Doc Example: {'text': 'OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.', 'title': 'Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia'}\n",
      "2023-05-26 21:28:48 - Loading Queries...\n",
      "2023-05-26 21:28:48 - Loaded 50 TEST Queries.\n",
      "2023-05-26 21:28:48 - Query Example: what is the origin of COVID-19\n"
     ]
    }
   ],
   "source": [
    "documents, data_path = io.get_data() #trec-covid as default\n",
    "samples_dir = os.path.join(data_path, \"samples\")\n",
    "tfidf_dir = os.path.join(samples_dir, \"tfidf\")\n",
    "if not os.path.exists(tfidf_dir):\n",
    "    os.mkdir(tfidf_dir)\n",
    "\n",
    "tfidf_results_path = os.path.join(tfidf_dir, \"tfidf_docs.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to vectorize longest_docs: 3.14 seconds\n",
      "Time taken to vectorize shortest_docs: 0.07 seconds\n",
      "Time taken to vectorize random_docs: 0.69 seconds\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(tfidf_results_path):\n",
    "    # tfidf_docs = []\n",
    "    # for name, docs, idx_to_id in tqdm(tokenized_samples):\n",
    "    #     vectorized_docs, time_taken = pp.vectorize(docs)\n",
    "    #     tfidf_docs.append((name, vectorized_docs, time_taken, idx_to_id))\n",
    "    #\n",
    "    # with open(tfidf_results_path, \"wb\") as f:\n",
    "    #     pickle.dump(tfidf_docs, f)\n",
    "    print(\"problem\")\n",
    "else:\n",
    "    with open(tfidf_results_path, \"rb\") as f:\n",
    "        tfidf_docs = pickle.load(f)\n",
    "for name, _, time_taken, _ in tfidf_docs:\n",
    "    print(f\"Time taken to vectorize {name}: {time_taken:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2023-05-26 21:37:12.676\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mcreate_spark_session\u001B[0m:\u001B[36m7\u001B[0m - \u001B[34m\u001B[1mExecutor memory: 1\u001B[0m\n",
      "\u001B[32m2023-05-26 21:37:12.677\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mcreate_spark_session\u001B[0m:\u001B[36m8\u001B[0m - \u001B[34m\u001B[1mAVAILABLE_RAM_GB: 8\u001B[0m\n",
      "\u001B[32m2023-05-26 21:37:12.678\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mcreate_spark_session\u001B[0m:\u001B[36m9\u001B[0m - \u001B[34m\u001B[1mTotal executor memory: 6\u001B[0m\n",
      "\u001B[32m2023-05-26 21:37:12.678\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mcreate_spark_session\u001B[0m:\u001B[36m10\u001B[0m - \u001B[34m\u001B[1mExecutor cores: 0\u001B[0m\n",
      "23/05/26 21:37:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "\u001B[32m2023-05-26 21:37:12.710\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36m__main__\u001B[0m:\u001B[36mcreate_spark_session\u001B[0m:\u001B[36m31\u001B[0m - \u001B[34m\u001B[1mAdding /Users/a/GitHub/All-Pairs-Similarity/src.zip to SparkContext\u001B[0m\n",
      "23/05/26 21:37:12 WARN SparkContext: The path /Users/a/GitHub/All-Pairs-Similarity/src.zip has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 26\u001B[0m\n\u001B[1;32m     23\u001B[0m spark \u001B[38;5;241m=\u001B[39m create_spark_session(n_executors\u001B[38;5;241m=\u001B[39mn_execs, app_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMR-APDSS\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Create docs df from pandas instance with new context\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m docs_scores_df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocs_scores_pandas\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m docs_scores_df\u001B[38;5;241m.\u001B[39mcache()\n\u001B[1;32m     29\u001B[0m mr_res \u001B[38;5;241m=\u001B[39m mr\u001B[38;5;241m.\u001B[39mMapReduceAPDSS()\u001B[38;5;241m.\u001B[39mapdss(\n\u001B[1;32m     30\u001B[0m     spark\u001B[38;5;241m=\u001B[39mspark,\n\u001B[1;32m     31\u001B[0m     docs_scores_df\u001B[38;5;241m=\u001B[39mdocs_scores_df,\n\u001B[1;32m     32\u001B[0m     threshold\u001B[38;5;241m=\u001B[39mt,\n\u001B[1;32m     33\u001B[0m     num_partitions\u001B[38;5;241m=\u001B[39mN_PARTITIONS\n\u001B[1;32m     34\u001B[0m )\n",
      "File \u001B[0;32m~/miniforge3/envs/sbruch-assignment-Lwmd/lib/python3.10/site-packages/pyspark/sql/session.py:1273\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1269\u001B[0m     data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(data, columns\u001B[38;5;241m=\u001B[39mcolumn_names)\n\u001B[1;32m   1271\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1272\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[0;32m-> 1273\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mSparkSession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[call-overload]\u001B[39;49;00m\n\u001B[1;32m   1274\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\n\u001B[1;32m   1275\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1276\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_dataframe(\n\u001B[1;32m   1277\u001B[0m     data, schema, samplingRatio, verifySchema  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   1278\u001B[0m )\n",
      "File \u001B[0;32m~/miniforge3/envs/sbruch-assignment-Lwmd/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:440\u001B[0m, in \u001B[0;36mSparkConversionMixin.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    438\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    439\u001B[0m converted_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_from_pandas(data, schema, timezone)\n\u001B[0;32m--> 440\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconverted_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/sbruch-assignment-Lwmd/lib/python3.10/site-packages/pyspark/sql/session.py:1318\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1316\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1317\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1318\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1319\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1320\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n",
      "File \u001B[0;32m~/miniforge3/envs/sbruch-assignment-Lwmd/lib/python3.10/site-packages/pyspark/sql/session.py:962\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    959\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(data)\n\u001B[1;32m    961\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 962\u001B[0m     struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inferSchemaFromList\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    963\u001B[0m     converter \u001B[38;5;241m=\u001B[39m _create_converter(struct)\n\u001B[1;32m    964\u001B[0m     tupled_data: Iterable[Tuple] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(converter, data)\n",
      "File \u001B[0;32m~/miniforge3/envs/sbruch-assignment-Lwmd/lib/python3.10/site-packages/pyspark/sql/session.py:836\u001B[0m, in \u001B[0;36mSparkSession._inferSchemaFromList\u001B[0;34m(self, data, names)\u001B[0m\n\u001B[1;32m    834\u001B[0m infer_array_from_first_element \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferArrayTypeFromFirstElement()\n\u001B[1;32m    835\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n\u001B[0;32m--> 836\u001B[0m schema \u001B[38;5;241m=\u001B[39m \u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    837\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_merge_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    838\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    839\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    840\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    841\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    842\u001B[0m \u001B[43m            \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    843\u001B[0m \u001B[43m            \u001B[49m\u001B[43minfer_array_from_first_element\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_array_from_first_element\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrow\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[1;32m    850\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome of types cannot be determined after inferring\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/sbruch-assignment-Lwmd/lib/python3.10/site-packages/pyspark/sql/session.py:839\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    834\u001B[0m infer_array_from_first_element \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf\u001B[38;5;241m.\u001B[39mlegacyInferArrayTypeFromFirstElement()\n\u001B[1;32m    835\u001B[0m prefer_timestamp_ntz \u001B[38;5;241m=\u001B[39m is_timestamp_ntz_preferred()\n\u001B[1;32m    836\u001B[0m schema \u001B[38;5;241m=\u001B[39m reduce(\n\u001B[1;32m    837\u001B[0m     _merge_type,\n\u001B[1;32m    838\u001B[0m     (\n\u001B[0;32m--> 839\u001B[0m         \u001B[43m_infer_schema\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    840\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    841\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    842\u001B[0m \u001B[43m            \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    843\u001B[0m \u001B[43m            \u001B[49m\u001B[43minfer_array_from_first_element\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_array_from_first_element\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    846\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m data\n\u001B[1;32m    847\u001B[0m     ),\n\u001B[1;32m    848\u001B[0m )\n\u001B[1;32m    849\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _has_nulltype(schema):\n\u001B[1;32m    850\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome of types cannot be determined after inferring\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/sbruch-assignment-Lwmd/lib/python3.10/site-packages/pyspark/sql/types.py:1573\u001B[0m, in \u001B[0;36m_infer_schema\u001B[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[0m\n\u001B[1;32m   1568\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m items:\n\u001B[1;32m   1569\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1570\u001B[0m         fields\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m   1571\u001B[0m             StructField(\n\u001B[1;32m   1572\u001B[0m                 k,\n\u001B[0;32m-> 1573\u001B[0m                 \u001B[43m_infer_type\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1574\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1575\u001B[0m \u001B[43m                    \u001B[49m\u001B[43minfer_dict_as_struct\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1576\u001B[0m \u001B[43m                    \u001B[49m\u001B[43minfer_array_from_first_element\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1577\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mprefer_timestamp_ntz\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1578\u001B[0m \u001B[43m                \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   1579\u001B[0m                 \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   1580\u001B[0m             )\n\u001B[1;32m   1581\u001B[0m         )\n\u001B[1;32m   1582\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1583\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to infer the type of the field \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/sbruch-assignment-Lwmd/lib/python3.10/site-packages/pyspark/sql/types.py:1447\u001B[0m, in \u001B[0;36m_infer_type\u001B[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001B[0m\n\u001B[1;32m   1442\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m DoubleType()\n\u001B[1;32m   1444\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1447\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_infer_type\u001B[39m(\n\u001B[1;32m   1448\u001B[0m     obj: Any,\n\u001B[1;32m   1449\u001B[0m     infer_dict_as_struct: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1450\u001B[0m     infer_array_from_first_element: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1451\u001B[0m     prefer_timestamp_ntz: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1452\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataType:\n\u001B[1;32m   1453\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Infer the DataType from obj\"\"\"\u001B[39;00m\n\u001B[1;32m   1454\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for dio_name, docs_, _, _ in tfidf_docs:\n",
    "    # # Create \"single-use\" spark session to parse the text\n",
    "    # spark = create_spark_session(n_executors=4, app_name=\"Doc Features\")\n",
    "    # docs_scores_df = tok.get_document_features(\n",
    "    #     spark=spark,\n",
    "    #     corpus_json_path=corpus_paths[d_name],\n",
    "    #     n_partitions=N_PARTITIONS\n",
    "    # )\n",
    "    if dio_name == \"shortest_docs\":\n",
    "        d_name = dio_name\n",
    "        diocan = docs_\n",
    "\n",
    "docs_scores_pandas: pd.DataFrame = pd.DataFrame(diocan.toarray())\n",
    "# spark.stop()\n",
    "\n",
    "\n",
    "n_executors: List[int] = [5]\n",
    "thresholds: List[float] = [0.7]\n",
    "\n",
    "for t in thresholds:\n",
    "    for n_execs in n_executors:\n",
    "        # ----- MR -----------------------------------\n",
    "        spark = create_spark_session(n_executors=n_execs, app_name=\"MR-APDSS\")\n",
    "\n",
    "        # Create docs df from pandas instance with new context\n",
    "        docs_scores_df = spark.createDataFrame(docs_scores_pandas)\n",
    "        docs_scores_df.cache()\n",
    "\n",
    "        mr_res = mr.MapReduceAPDSS().apdss(\n",
    "            spark=spark,\n",
    "            docs_scores_df=docs_scores_df,\n",
    "            threshold=t,\n",
    "            num_partitions=N_PARTITIONS\n",
    "        )\n",
    "\n",
    "        mr_setting = (d_name, n_execs, t)\n",
    "        logger.info(f\"MR::({mr_setting}):time -> {mr_res.time}\")\n",
    "        mr_results[mr_setting] = mr_res\n",
    "        save_mr_results()  # Overwrite each time to make sure data isn't lost\n",
    "\n",
    "        # Stop context so that next iteration creates a new one with\n",
    "        #   a different number of executors\n",
    "        spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mr_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_mr_results()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "sbruch-assignment-lwmd",
   "language": "python",
   "display_name": "lwmd_asgmt_sbruch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}